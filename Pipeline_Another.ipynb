{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "### our imports\n",
    "\n",
    "from src.utils import (get_data_from_directory, get_files_directory_list, \n",
    "                       one_hot_encoding, TimeSeriesDataset,get_device, train_clf)\n",
    "\n",
    "from src.TFE import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -nc \"http://www.timeseriesclassification.com/Downloads/Archives/Univariate2018_arff.zip\"\n",
    "#!unzip -q -n \"Univariate2018_arff.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  Coffee\n",
      "X_train shape:  (28, 286)\n",
      "y_train shape:  (28,)\n",
      "X_test shape:   (28, 286)\n",
      "y_test shape:   (28,)\n"
     ]
    }
   ],
   "source": [
    "directory_list = get_files_directory_list()\n",
    "directory_list = sorted(directory_list)\n",
    "\n",
    "random_index =  15 #6 # 5\n",
    "random_path = directory_list[random_index]\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_data_from_directory(random_path)\n",
    "X_train = X_train.squeeze()\n",
    "y_train = y_train.squeeze()\n",
    "X_test = X_test.squeeze()\n",
    "y_test = y_test.squeeze()\n",
    "\n",
    "print('Dataset: ', random_path)\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('X_test shape:  ', X_test.shape)\n",
    "print('y_test shape:  ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = TopologicalFeaturesExtractor(\n",
    "    persistence_diagram_extractor=PersistenceDiagramsExtractor(tokens_embedding_dim=3, \n",
    "                                                               tokens_embedding_delay=10,\n",
    "                                                               homology_dimensions=(0, 1, 2)),\n",
    "    persistence_diagram_features=[HolesNumberFeature(),\n",
    "                                  MaxHoleLifeTimeFeature(),\n",
    "                                  RelevantHolesNumber(),\n",
    "                                  AverageHoleLifetimeFeature(),\n",
    "                                  SumHoleLifetimeFeature(),\n",
    "                                  PersistenceEntropyFeature(),\n",
    "                                  SimultaneousAliveHolesFeatue()])\n",
    "\n",
    "X_train_transformed = feature_extractor.fit_transform(X_train)\n",
    "X_test_transformed = feature_extractor.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_transformed shape:  (28, 21)\n",
      "X_test_transformed shape:   (28, 21)\n"
     ]
    }
   ],
   "source": [
    "print('X_train_transformed shape: ', X_train_transformed.shape)\n",
    "print('X_test_transformed shape:  ', X_test_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.8928571428571429\n",
      "Test accuracy:  0.6428571428571429\n"
     ]
    }
   ],
   "source": [
    "parameters = {\"C\": [10**i for i in range(-2, 5)],\n",
    "              \"kernel\": [\"linear\", \"rbf\", \"sigmoid\", \"poly\"]}\n",
    "\n",
    "svc_cv = GridSearchCV(SVC(random_state=42), \n",
    "                      param_grid=parameters,\n",
    "                      cv=5,\n",
    "                      scoring='accuracy', \n",
    "                      n_jobs=-1)\n",
    "svc_cv.fit(X_train_transformed, y_train)\n",
    "\n",
    "print(\"Train accuracy: \", accuracy_score(y_train, svc_cv.best_estimator_.predict(X_train_transformed)))\n",
    "print(\"Test accuracy: \", accuracy_score(y_test, svc_cv.best_estimator_.predict(X_test_transformed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.9642857142857143\n",
      "Test accuracy:  0.7857142857142857\n"
     ]
    }
   ],
   "source": [
    "parameters = {\"max_depth\": [2, 10, 15, 20, 25, 30, 35, 40, 45, 50, 70, 100, 120, 150],\n",
    "              \"n_estimators\": [20, 50, 100, 150, 200, 250]}\n",
    "svc_cv = GridSearchCV(XGBClassifier(n_jobs=-1, random_state=42), \n",
    "                      param_grid=parameters,\n",
    "                      cv=2,\n",
    "                      scoring='accuracy', \n",
    "                      n_jobs=-1)\n",
    "svc_cv.fit(X_train_transformed, y_train)\n",
    "\n",
    "print(\"Train accuracy: \", accuracy_score(y_train, svc_cv.best_estimator_.predict(X_train_transformed)))\n",
    "print(\"Test accuracy: \", accuracy_score(y_test, svc_cv.best_estimator_.predict(X_test_transformed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.8214285714285714\n",
      "Test accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "parameters = {\"n_neighbors\": [3, 5, 7, 11,]}\n",
    "\n",
    "knn_cv = GridSearchCV(KNeighborsClassifier(n_jobs=-1), \n",
    "                      param_grid=parameters,\n",
    "                      cv=5,\n",
    "                      scoring='accuracy', \n",
    "                      n_jobs=-1)\n",
    "knn_cv.fit(X_train_transformed, y_train)\n",
    "\n",
    "print(\"Train accuracy: \", accuracy_score(y_train, knn_cv.best_estimator_.predict(X_train_transformed)))\n",
    "print(\"Test accuracy: \", accuracy_score(y_test, knn_cv.best_estimator_.predict(X_test_transformed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dim = np.unique(y_train).shape[0]\n",
    "device = get_device()\n",
    "handle_dim = lambda x: np.swapaxes(x[..., np.newaxis], 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed_dim = handle_dim(X_train_transformed)\n",
    "X_test_transformed_dim  = handle_dim(X_test_transformed)\n",
    "\n",
    "y_hot_train = one_hot_encoding(y_train)\n",
    "y_hot_test = one_hot_encoding(y_test)\n",
    "\n",
    "dataset_train = TimeSeriesDataset(X_train_transformed_dim, y_hot_train)\n",
    "dataset_test  = TimeSeriesDataset(X_test_transformed_dim, y_hot_test)\n",
    "\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Conv1d(1, 32, 3),\n",
    "                      nn.MaxPool1d(2),\n",
    "                      nn.ReLU(),\n",
    "                      \n",
    "                      nn.Conv1d(32, 32, 4),\n",
    "                      nn.MaxPool1d(2),\n",
    "                      nn.ReLU(),\n",
    "                      \n",
    "                      nn.Conv1d(32, 16, 3, 2),\n",
    "                      nn.ReLU(),\n",
    "                      \n",
    "                      nn.Flatten(),\n",
    "                      \n",
    "                      nn.Linear(16, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 32),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(32, out_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv1d(1, 32, kernel_size=(3,), stride=(1,))\n",
       "  (1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (2): ReLU()\n",
       "  (3): Conv1d(32, 32, kernel_size=(4,), stride=(1,))\n",
       "  (4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): ReLU()\n",
       "  (6): Conv1d(32, 16, kernel_size=(3,), stride=(2,))\n",
       "  (7): ReLU()\n",
       "  (8): Flatten()\n",
       "  (9): Linear(in_features=16, out_features=64, bias=True)\n",
       "  (10): ReLU()\n",
       "  (11): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (12): ReLU()\n",
       "  (13): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.apply(init_weights).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250/5000 ||\t Loss:  Train 0.2234 | Validation 0.2467\n",
      "Epoch 500/5000 ||\t Loss:  Train 0.0110 | Validation 0.1176\n",
      "Epoch 750/5000 ||\t Loss:  Train 0.0010 | Validation 0.1372\n",
      "Epoch 1000/5000 ||\t Loss:  Train 0.0000 | Validation 0.1450\n",
      "Epoch 1250/5000 ||\t Loss:  Train 0.0000 | Validation 0.1454\n",
      "Epoch 1500/5000 ||\t Loss:  Train 0.0000 | Validation 0.1455\n",
      "Epoch 1750/5000 ||\t Loss:  Train 0.0000 | Validation 0.1455\n",
      "Epoch 2000/5000 ||\t Loss:  Train 0.0000 | Validation 0.1456\n",
      "Epoch 2250/5000 ||\t Loss:  Train 0.0000 | Validation 0.1456\n",
      "Epoch 2500/5000 ||\t Loss:  Train 0.0000 | Validation 0.1456\n",
      "Epoch 2750/5000 ||\t Loss:  Train 0.0000 | Validation 0.1456\n",
      "Epoch 3000/5000 ||\t Loss:  Train 0.0000 | Validation 0.1456\n",
      "Epoch 3250/5000 ||\t Loss:  Train 0.0000 | Validation 0.1456\n",
      "Epoch 3500/5000 ||\t Loss:  Train 0.0000 | Validation 0.1456\n",
      "Epoch 3750/5000 ||\t Loss:  Train 0.0000 | Validation 0.1456\n",
      "Epoch 4000/5000 ||\t Loss:  Train 0.0000 | Validation 0.1456\n",
      "Epoch 4250/5000 ||\t Loss:  Train 0.0000 | Validation 0.1456\n",
      "Epoch 4500/5000 ||\t Loss:  Train 0.0000 | Validation 0.1456\n",
      "Epoch 4750/5000 ||\t Loss:  Train 0.0000 | Validation 0.1456\n",
      "Epoch 5000/5000 ||\t Loss:  Train 0.0000 | Validation 0.1455\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 5000\n",
    "lr = 1e-2\n",
    "t_max = np.var(X_train_transformed)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, )\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, t_max)\n",
    "\n",
    "train_clf(num_epoch, model, criterion, optimizer, loader_train, loader_test, scheduler) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred = lambda x: model(x.dataset[:][0]).round().abs().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  1.0\n",
      "Test accuracy:  0.8214285714285714\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy: \", accuracy_score(y_hot_train, model_pred(loader_train)))\n",
    "print(\"Test accuracy: \", accuracy_score(y_hot_test, model_pred(loader_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}